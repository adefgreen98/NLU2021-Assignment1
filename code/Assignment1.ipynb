{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO8OGnrhe1KP5LJuEjNuxmO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adefgreen98/NLU2021-Assignment1/blob/main/code/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcPD_mJOSZRp"
      },
      "source": [
        "# Natural Language Understanging Assignment 1 - Dependency Grammars\n",
        "_Federico Pedeni_, 223993"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZDyJJ6uSl_S"
      },
      "source": [
        "### Requirements & Test Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daDS5x3uSlTS"
      },
      "source": [
        "import spacy\n",
        "from typing import Union\n",
        "\n",
        "nlp_spacy = spacy.load('en')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHsOwNc3h5Sh"
      },
      "source": [
        "sentences = \"i saw the man with the telescope\"\n",
        "doc_spacy = nlp_spacy(sentences)\n",
        "\n",
        "test_sentences = {\n",
        "    'subtree': [\"saw\", \"I saw a woman that saw a man who saw me yesterday\"],\n",
        "    'check': [\"with the telescope\", \"telescope with the\"],\n",
        "    'head': \"the man with the telescope\",\n",
        "    'info': \"I gave Pooh all my honey. Also, I told Tigger a good bedtime story. Cristopher Robin was relieved to see all these friends.\"\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X410esaxRXGd"
      },
      "source": [
        "### 1) Extract a path of dependency relations from ROOT to a token\n",
        "This function extracts a list of paths that allow to traverse the dependency graph from the root to each node of the tree. Each dependency relation is stated as a `3-tuple` containing `(head, child, type of dependency)`; for this reason, for each list, the `child` value of a tuple is also the `head` value of the subsequent tuple.\n",
        "\n",
        "For each sentence in the given SpaCy `Doc`, it is created a `dict` that maps each token to its path; for the `root` node it is reported only a recursive relation. Tokens that occurr multiple times inside a sentence are distinguished thanks to __token offsets__ that are part of the dictionary's keys.\n",
        "\n",
        "Dictionaries' values are lists of dependency relations: while iterating over a sentence, for each key is initialized an empty list and new relations are progressively addedd in a backwards-recursive manner, substituting the token with its head until the root node is reached. At this point, the root-to-itself relation is added and the list is reversed.\n",
        "\n",
        "This function returns a list where dictionaries at each index refer to the corresponding sentences in the `Doc`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi7UHzJvRlfs",
        "outputId": "916982a5-b2fd-4d60-90af-ec37aafc9e0b"
      },
      "source": [
        "def get_dependencies(doc:spacy.tokens.Doc=doc_spacy):\n",
        "    # iterates over all sentences in the doc\n",
        "    rlist = []\n",
        "    for sentence in doc.sents:\n",
        "        res = {}\n",
        "        rt = sentence.root\n",
        "        # gets offset for the sentence, so that keys will be indexed independently for each sentence\n",
        "        offset = sentence[0].i\n",
        "        for wd in sentence:\n",
        "            token = wd\n",
        "            # forms the key\n",
        "            k = wd.text + f'<{wd.i - offset}>'\n",
        "            res[k] = []\n",
        "            while rt != token:\n",
        "                res[k].append((token.head.text, token.text, token.dep_))\n",
        "                token = token.head\n",
        "            # at this point the root should add itself\n",
        "            res[k].append((token.head.text, token.text, token.dep_))\n",
        "            res[k].reverse()\n",
        "        rlist.append(res)\n",
        "    return rlist\n",
        "\n",
        "print(f\"Dependencies for sentence '{doc_spacy.text}'\")\n",
        "print(*get_dependencies()[0].items(), sep='\\n')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dependencies for sentence 'i saw the man with the telescope'\n",
            "('i<0>', [('saw', 'saw', 'ROOT'), ('saw', 'i', 'nsubj')])\n",
            "('saw<1>', [('saw', 'saw', 'ROOT')])\n",
            "('the<2>', [('saw', 'saw', 'ROOT'), ('saw', 'man', 'dobj'), ('man', 'the', 'det')])\n",
            "('man<3>', [('saw', 'saw', 'ROOT'), ('saw', 'man', 'dobj')])\n",
            "('with<4>', [('saw', 'saw', 'ROOT'), ('saw', 'man', 'dobj'), ('man', 'with', 'prep')])\n",
            "('the<5>', [('saw', 'saw', 'ROOT'), ('saw', 'man', 'dobj'), ('man', 'with', 'prep'), ('with', 'telescope', 'pobj'), ('telescope', 'the', 'det')])\n",
            "('telescope<6>', [('saw', 'saw', 'ROOT'), ('saw', 'man', 'dobj'), ('man', 'with', 'prep'), ('with', 'telescope', 'pobj')])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QrCErJhRk-T"
      },
      "source": [
        "### 2) Extract a subtree of dependants given a token\n",
        "\n",
        "This exercise can be solved by creating a wrapper function accepting as parameter a `Span` object, so that it can be used again in exercise 3. \n",
        "\n",
        "Indeed, the external function does only the sentence parsing and the detection of specifed token in the `Doc` sentences. Since a token can appear multiple times in a sentence, its occurrences are distinguished by their offset (as in exercise 1) and the relative subtrees are returned as a mappings between the __token-occurrence string__ and the __subtree__ of dependants, representend as list of `Token` objects (ordered according to sentence order). Matches between dictionary's keys are computed by taking the first part of the dict's key (the one that comes before the offset specification).\n",
        "\n",
        "The internal function accepts as input a single sentence and initializes a dict that is used to map each token in the sentence with its subtree. Each subtree is representend as a __list of `Token`__ objects ordered according to sentence order. These are obtained through a BFS-like exploration of the sentence graph: at each step, a queue is filled with a token's children (excluding the token itself) and these will be explored later, in discovering order. The exploration continues until leaves are met, which do not add any child to the queue and therefore terminate the scan. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "168sZ6DBRscB",
        "outputId": "6a8ec579-efb0-4cb2-8ba4-c724e04292e2"
      },
      "source": [
        "def _get_subtree(doc:spacy.tokens.Span):\n",
        "    rsdict = {\n",
        "        doc.root.text + f'<{doc.root.i}>': list(doc.root.subtree)\n",
        "    }\n",
        "    q = list(doc.root.children)\n",
        "    while len(q) > 0:\n",
        "        token = q.pop(0)\n",
        "        rsdict[token.text + f'<{token.i}>'] = list(token.subtree)\n",
        "        q.extend(filter(lambda x: x.i != token.i, token.subtree))\n",
        "    return rsdict\n",
        "\n",
        "def get_subtree(token:str, doc:str, parser=nlp_spacy):\n",
        "    _doc = parser(doc)\n",
        "    res = []\n",
        "    for sent in _doc.sents:\n",
        "        res.append({})\n",
        "        for k,v in _get_subtree(sent).items():\n",
        "            if k.split('<')[0] == token:\n",
        "                # fills the last added dictionary (for the current sentence) with subtree list\n",
        "                res[-1][k] = v\n",
        "    return res\n",
        "\n",
        "print(f\"Subtree for token '{test_sentences['subtree'][0]}' in sentence '{test_sentences['subtree'][1]}'\")\n",
        "print(*get_subtree(test_sentences['subtree'][0], test_sentences['subtree'][1])[0].items(), sep='\\n')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Subtree for token 'saw' in sentence 'I saw a woman that saw a man who saw me yesterday'\n",
            "('saw<1>', [I, saw, a, woman, that, saw, a, man, who, saw, me, yesterday])\n",
            "('saw<5>', [that, saw, a, man, who, saw, me, yesterday])\n",
            "('saw<9>', [who, saw, me, yesterday])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDwi_x29Rsx0"
      },
      "source": [
        "### 3) Check if a given list of tokens (segment of a sentence) forms a subtree\n",
        "\n",
        "This function accepts a list of tokens (that can be specified as a string with spaces between each token, too) and verifies if they are the _all and only_ components of a subtree which is contained inside a specified `Doc`. For first, it iterates over all the sentences in the `Doc`, trying to find if anyone of them contains all the specified tokens; if not, then they surely do not form a subtree for any sentence and thus it is returned `False`.\n",
        "\n",
        "If at least one suitable sentence is found, then it iterates over all the subtrees of that sentence, obtained thanks to the internal function of exercise 2, and return `True` if it finds one where the tokens' ordering matches the sentence ordering, by comparing the tokens' list and the list of `Token.text` for each subtree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVzKjq21Rxli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f576096b-40b5-4c80-d865-2345cba25b7f"
      },
      "source": [
        "def check_subtree(tokens:Union[list, str], doc:spacy.tokens.Doc=doc_spacy):\n",
        "    if type(tokens) is str: tokens = tokens.split()\n",
        "    else: pass\n",
        "    token_pool = set(tokens)\n",
        "\n",
        "    for sentence in doc.sents:\n",
        "        acc = token_pool.issubset(set(sentence.text.split()))\n",
        "        if acc == True: \n",
        "            for subtr in _get_subtree(sentence).values():\n",
        "                subtr = [el.text for el in subtr]\n",
        "                if subtr == tokens: return True\n",
        "    return False\n",
        "\n",
        "for sent in test_sentences['check']:    \n",
        "    print(f\"Test for check_subtree('{sent}')\", \" ---> \", check_subtree(sent))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test for check_subtree('with the telescope')  --->  True\n",
            "Test for check_subtree('telescope with the')  --->  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdIvy6hYRyeZ"
      },
      "source": [
        "### 4) Identify head of a span, given its tokens\n",
        "\n",
        "In this case, the input has been considered to be a string; the function also needs a pre-initialized parser, to perform the sentence parsing.\n",
        "\n",
        "This function creates a new `Doc` containing the specified span and then returns the root token of the single `Span` object that composes that `Doc`, by accessing it directly thanks to __Python slicing__. \n",
        "\n",
        "The returned value is a `Token` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCbWah_pR3Jd",
        "outputId": "7fc69d22-3f90-4ecf-dbff-051364d71950"
      },
      "source": [
        "def head_of_span(sentence:str, parser=nlp_spacy):\n",
        "    tmp = parser(sentence)\n",
        "    return tmp[:].root\n",
        "\n",
        "print(f\"Head of sentence '{test_sentences['head']}': \", head_of_span(test_sentences['head']))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Head of sentence 'the man with the telescope':  man\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8YCGd0vR3eJ"
      },
      "source": [
        "### 5) Extract sentence subject, direct object and indirect object spans\n",
        "\n",
        "First, this function defines a mapping between some types of dependency relations that match the 3 requested categories and the categories themselves. \n",
        "\n",
        "After that, it iterates over each word of wach sentence of the `Doc` and populates a dictionary with the spans for each category. Each span is obtained by taking the subtree of a token (by casting to list the `Token.subtree` object) that has a specific dependency relation; in particular, these are the relations used to classify as subject, object and indirect object:\n",
        "+ subject: `nsubj`, `nsubjpass`;\n",
        "+ object: `dobj`;\n",
        "+ indirect object: `dative`.\n",
        "\n",
        "Returns a list indexed by each sentence in the `Doc`, where each element is a mapping from the requested object to the spans (list of `Token` objects ordered according to sentence order) that match the corresponding dependency relation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYv3V7CYR8Nh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a146221-eeca-41d0-a6cf-ff44f2470b35"
      },
      "source": [
        "def extract_info(doc:spacy.tokens.Doc):\n",
        "    relations = {\n",
        "        'nsubj': 'subject',\n",
        "        'nsubjpass': 'subject',\n",
        "        'dobj': 'object',\n",
        "        'dative': 'indirect object',\n",
        "    }\n",
        "    res = []\n",
        "    for sentence in doc.sents:\n",
        "        tmp = {\n",
        "        'subject': [],\n",
        "        'object': [],\n",
        "        'indirect object': []\n",
        "        }\n",
        "        for word in sentence:\n",
        "            if word.dep_ in relations.keys():\n",
        "                tmp[relations[word.dep_]].append(list(word.subtree))\n",
        "        res.append(tmp)\n",
        "    return res\n",
        "\n",
        "for sent, info in zip(nlp_spacy(test_sentences['info']).sents, extract_info(nlp_spacy(test_sentences['info']))):\n",
        "    print(\"Current sentence: \", sent)\n",
        "    print(\"Info: \")\n",
        "    print(*info.items(), sep='\\n')\n",
        "    print()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current sentence:  I gave Pooh all my honey.\n",
            "Info: \n",
            "('subject', [I])\n",
            "('object', [all, my, honey])\n",
            "('indirect object', [Pooh])\n",
            "\n",
            "Current sentence:  Also, I told Tigger a good bedtime story.\n",
            "Info: \n",
            "('subject', [I])\n",
            "('object', [a, good, bedtime, story])\n",
            "('indirect object', [Tigger])\n",
            "\n",
            "Current sentence:  Cristopher Robin was relieved to see all these friends.\n",
            "Info: \n",
            "('subject', [Cristopher, Robin])\n",
            "('object', [all, these, friends])\n",
            "('indirect object', None)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woPU09Z6yW6T"
      },
      "source": [
        "# Optional Part\n",
        "\n",
        "Not done."
      ]
    }
  ]
}