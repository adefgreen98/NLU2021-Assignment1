{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "80UieR6O1xER"
      ],
      "authorship_tag": "ABX9TyNQfqQqwd6qFHh2/1AJZn7Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adefgreen98/NLU2021-Assignment1/blob/main/code/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcPD_mJOSZRp"
      },
      "source": [
        "# Natural Language Understanging Assignment 1 - Dependency Grammars\n",
        "_Federico Pedeni_, 223993"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZDyJJ6uSl_S"
      },
      "source": [
        "### Requirements & Test Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daDS5x3uSlTS"
      },
      "source": [
        "import spacy\n",
        "from typing import Union\n",
        "\n",
        "nlp_spacy = spacy.load('en')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHsOwNc3h5Sh"
      },
      "source": [
        "sentences = \"i saw the man with the telescope\"\n",
        "doc_spacy = nlp_spacy(sentences)\n",
        "\n",
        "test_sentences = {\n",
        "    'subtree': [\"saw\", \"I saw a woman that saw a man who saw me yesterday\"],\n",
        "    'check': [\"with the telescope\", \"telescope with the\"],\n",
        "    'head': \"the man with the telescope\",\n",
        "    'info': \"I gave Pooh all my honey. Also, I told Tigger a good bedtime story. Cristopher Robin was relieved to see all these friends.\"\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X410esaxRXGd"
      },
      "source": [
        "### 1) Extract a path of dependency relations from ROOT to a token\n",
        "This function extracts a list of paths that allow to traverse the dependency graph from the root to each node of the tree. Each dependency relation is stated as a `3-tuple` containing `(head, child, type of dependency)`; for this reason, for each list, the `child` value of a tuple is also the `head` value of the subsequent tuple.\n",
        "\n",
        "For each sentence in the given SpaCy `Doc`, it is created a `dict` that maps each token to its path; for the `root` node it is reported only a recursive relation. Tokens that occurr multiple times inside a sentence are distinguished thanks to __token offsets__ that are part of the dictionary's keys.\n",
        "\n",
        "Dictionaries' values are lists of dependency relations: while iterating over a sentence, for each key is initialized an empty list and new relations are progressively addedd in a backwards-recursive manner, substituting the token with its head until the root node is reached. At this point, the root-to-itself relation is added and the list is reversed.\n",
        "\n",
        "This function returns a list where dictionaries at each index refer to the corresponding sentences in the `Doc`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi7UHzJvRlfs",
        "outputId": "6b1aa944-6c03-4f8a-82c4-3d82974720c3"
      },
      "source": [
        "def get_dependencies(doc:spacy.tokens.Doc=doc_spacy):\n",
        "    # iterates over all sentences in the doc\n",
        "    rlist = []\n",
        "    for sentence in doc.sents:\n",
        "        res = {}\n",
        "        rt = sentence.root\n",
        "        # gets offset for the sentence, so that keys will be indexed independently for each sentence\n",
        "        offset = sentence[0].i\n",
        "        for wd in sentence:\n",
        "            token = wd\n",
        "            # forms the key\n",
        "            k = wd.text + f'<{wd.i - offset}>'\n",
        "            res[k] = []\n",
        "            while rt != token:\n",
        "                res[k].append((token.head.text, token.text, token.dep_))\n",
        "                token = token.head\n",
        "            # at this point the root should add itself\n",
        "            res[k].append((token.head.text, token.text, token.dep_))\n",
        "            res[k].reverse()\n",
        "        rlist.append(res)\n",
        "    return rlist\n",
        "\n",
        "print(*get_dependencies()[0].items(), sep='\\n')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('i<0>', [('saw', 'saw', 'ROOT'), ('saw', 'i', 'nsubj')])\n",
            "('saw<1>', [('saw', 'saw', 'ROOT')])\n",
            "('the<2>', [('saw', 'saw', 'ROOT'), ('saw', 'man', 'dobj'), ('man', 'the', 'det')])\n",
            "('man<3>', [('saw', 'saw', 'ROOT'), ('saw', 'man', 'dobj')])\n",
            "('with<4>', [('saw', 'saw', 'ROOT'), ('saw', 'man', 'dobj'), ('man', 'with', 'prep')])\n",
            "('the<5>', [('saw', 'saw', 'ROOT'), ('saw', 'man', 'dobj'), ('man', 'with', 'prep'), ('with', 'telescope', 'pobj'), ('telescope', 'the', 'det')])\n",
            "('telescope<6>', [('saw', 'saw', 'ROOT'), ('saw', 'man', 'dobj'), ('man', 'with', 'prep'), ('with', 'telescope', 'pobj')])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QrCErJhRk-T"
      },
      "source": [
        "### 2) Extract a subtree of dependants given a token\n",
        "\n",
        "This exercise can be solved by creating a wrapper function accepting as parameter a `Span` object, so that it can be used again in exercise 3. \n",
        "\n",
        "Indeed, the external function does only the sentence parsing and the detection of specifed token in the `Doc` sentences. Since a token can appear multiple times in a sentence, its occurrence are distinguished through their offset (as in exercise 1) and the relative subtrees are returned as a mapping between the __token-occurrence string__ and the __subtree__ of dependants, representend as list of `Token` objects (ordered according to sentence order). Matches between dictionary's keys are computed by taking the first part of the dict's key (the one that comes before the offset specification).\n",
        "\n",
        "The internal function accepts as input a single sentence and initializes a dict that is used to map each token in the sentence with its subtree. Each subtree is representend as a __list of `Token`__ objects ordered according to sentence order: these are obtained thanks to a BFS-like exploration of the sentence graph; at each step, a queue is filled with a token's children (excluding the token itself) and these will be explored later in discovering order. The exploration continues until leaves are met, which do not add any child to the queue and therefore terminate the scan. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "168sZ6DBRscB",
        "outputId": "14f4fa2a-af7c-49d5-e794-075d082e86c1"
      },
      "source": [
        "def _get_subtree(doc:spacy.tokens.Span):\n",
        "    rsdict = {\n",
        "        doc.root.text + f'<{doc.root.i}>': list(doc.root.subtree)\n",
        "    }\n",
        "    q = list(doc.root.children)\n",
        "    while len(q) > 0:\n",
        "        token = q.pop(0)\n",
        "        rsdict[token.text + f'<{token.i}>'] = list(token.subtree)\n",
        "        q.extend(filter(lambda x: x.i != token.i, token.subtree))\n",
        "    return rsdict\n",
        "\n",
        "def get_subtree(token:str, doc:str, parser=nlp_spacy):\n",
        "    _doc = parser(doc)\n",
        "    res = []\n",
        "    for sent in _doc.sents:\n",
        "        res.append({})\n",
        "        for k,v in _get_subtree(sent).items():\n",
        "            if k.split('<')[0] == token:\n",
        "                # fills the last added dictionary (for the current sentence) with subtree list\n",
        "                res[-1][k] = v\n",
        "    return res\n",
        "    \n",
        "print(*get_subtree(test_sentences['subtree'][0], test_sentences['subtree'][1])[0].items(), sep='\\n')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('saw<1>', [I, saw, a, woman, that, saw, a, man, who, saw, me, yesterday])\n",
            "('saw<5>', [that, saw, a, man, who, saw, me, yesterday])\n",
            "('saw<9>', [who, saw, me, yesterday])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDwi_x29Rsx0"
      },
      "source": [
        "### 3) Check if a given list of tokens (segment of a sentence) forms a subtree\n",
        "\n",
        "This function accepts a list of tokens (that can be specified as a string with spaces between each token, too) to verify if they are the _all and only_ components of a subtree which is contained inside a specified `Doc`. For first, it iterates over all the sentences in the `Doc`, trying to find if anyone of them contains all the specified tokens; if not, then they surely do not form a subtree for any sentence and thus it is returned `False`.\n",
        "\n",
        "If at least one suitable sentence is found, then it iterates over all the subtrees of that sentence and return `True` if it finds one where the tokens' ordering matches the sentence ordering, by comparing the tokens' list and the list of `Token.text` for each subtree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVzKjq21Rxli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "debc8bca-6042-4ba3-872b-8e2f18f86f82"
      },
      "source": [
        "def check_subtree(tokens:Union[list, str], doc:spacy.tokens.Doc=doc_spacy):\n",
        "    if type(tokens) is str: tokens = tokens.split()\n",
        "    else: pass\n",
        "    token_pool = set(tokens)\n",
        "\n",
        "    for sentence in doc.sents:\n",
        "        acc = token_pool.issubset(set(sentence.text.split()))\n",
        "        if acc == True: \n",
        "            for subtr in _get_subtree(sentence).values():\n",
        "                subtr = [el.text for el in subtr]\n",
        "                if subtr == tokens: return True\n",
        "    return False\n",
        "\n",
        "for sent in test_sentences['check']:    \n",
        "    print(f\"Test for check_subtree('{sent}')\", \" ---> \", check_subtree(sent))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test for check_subtree('with the telescope')  --->  True\n",
            "Test for check_subtree('telescope with the')  --->  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdIvy6hYRyeZ"
      },
      "source": [
        "### 4) Identify head of a span, given its tokens\n",
        "\n",
        "In this case, the input has been considered to be a string; the function also needs a pre-initialized parser, to perform the sentence parsing.\n",
        "\n",
        "This function creates a new `Doc` containing the specified span and then returns the root token of the single `Span` object that composes that `Doc`, by accessing it directly thanks to __Python slicing__. \n",
        "\n",
        "The returned value is a `Token` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCbWah_pR3Jd",
        "outputId": "f9f5fea8-f750-4d5f-eb26-befaf0fd18a7"
      },
      "source": [
        "def head_of_span(sentence:str, parser=nlp_spacy):\n",
        "    tmp = parser(sentence)\n",
        "    return tmp[:].root\n",
        "\n",
        "print(f\"Head of sentence '{test_sentences['head']}': \", head_of_span(test_sentences['head']))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Head of sentence 'the man with the telescope':  man\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8YCGd0vR3eJ"
      },
      "source": [
        "### 5) Extract sentence subject, direct object and indirect object spans\n",
        "\n",
        "This function defines a mapping between some types of dependency relations and the 3 requested categories. \n",
        "\n",
        "Going into details, a span is added to the corresponding list if and only if its head is labelled with one of the following dependencies:\n",
        "+ subject: `nsubj`, `nsubjpass`;\n",
        "+ object: `dobj`;\n",
        "+ indirect object: `pobj`, `iobj`, `dative`.\n",
        "\n",
        "The function iterates over the sentences in the `Doc` and for each one creates a dictionary mapping each requested dependency relation to the span of words in the __subtree__ of the token labeled with that relation. The subtree is obtained by casting to list the generator stored in `Token.subtree`. \n",
        "\n",
        "Since there may be multiple indirect objects, the values of the dictionary are implemented as lists (even for subject and direct object).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYv3V7CYR8Nh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c4db95-091c-4924-a2f6-a97f6c26cbc2"
      },
      "source": [
        "def extract_info(doc:spacy.tokens.Doc):\n",
        "    relations = {\n",
        "        'nsubj': 'subject',\n",
        "        'nsubjpass': 'subject',\n",
        "        'dobj': 'object',\n",
        "        'pobj': 'indirect object',\n",
        "        'dative': 'indirect object',\n",
        "        'iobj': 'indirect object'\n",
        "    }\n",
        "    res = []\n",
        "    for sentence in doc.sents:\n",
        "        tmp = {\n",
        "        'subject': None,\n",
        "        'object': None,\n",
        "        'indirect object': None\n",
        "        }\n",
        "        for word in sentence:\n",
        "            if word.dep_ in relations.keys():\n",
        "                tmp[relations[word.dep_]] = (list(word.subtree), word.dep_)\n",
        "        res.append(tmp)\n",
        "    return res\n",
        "\n",
        "for sent, info in zip(nlp_spacy(test_sentences['info']).sents, extract_info(nlp_spacy(test_sentences['info']))):\n",
        "    print(\"Current sentence: \", sent)\n",
        "    print(\"Info: \")\n",
        "    print(*info.items(), sep='\\n')\n",
        "    print()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current sentence:  I gave Pooh all my honey.\n",
            "Info: \n",
            "('subject', ([I], 'nsubj'))\n",
            "('object', ([all, my, honey], 'dobj'))\n",
            "('indirect object', ([Pooh], 'dative'))\n",
            "\n",
            "Current sentence:  Also, I told Tigger a good bedtime story.\n",
            "Info: \n",
            "('subject', ([I], 'nsubj'))\n",
            "('object', ([a, good, bedtime, story], 'dobj'))\n",
            "('indirect object', ([Tigger], 'dative'))\n",
            "\n",
            "Current sentence:  Cristopher Robin was relieved to see all these friends.\n",
            "Info: \n",
            "('subject', ([Cristopher, Robin], 'nsubjpass'))\n",
            "('object', ([all, these, friends], 'dobj'))\n",
            "('indirect object', None)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvdYRGbQR8tZ"
      },
      "source": [
        "## Optional Section\n",
        "### 1) Modify NLTK Transition parser's `Configuration` class to use better features\n",
        "### 2) Evaluate the features comparing performances\n",
        "### 3) Replace SVM classifier with an alternative\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vKGyGIJPC4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74c70b8-70be-42a4-f9cf-6eafed739d7f"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import dependency_treebank\n",
        "from nltk.parse import DependencyEvaluator\n",
        "from nltk.parse.transitionparser import *\n",
        "\n",
        "nltk.download('dependency_treebank') # 4k samples\n",
        "origninal_tp = TransitionParser('arc-standard')\n",
        "sentences = dependency_treebank.parsed_sents()[:150]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package dependency_treebank to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/dependency_treebank.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDWuNjXp0LDT",
        "outputId": "54806f97-1270-4c40-e39b-fc0947762d6c"
      },
      "source": [
        "origninal_tp.train(sentences, 'original_tp.model')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Number of training examples : 150\n",
            " Number of valid (projective) examples : 150\n",
            "[LibSVM]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p78vUOI0OqgF",
        "outputId": "833ded15-cf10-4bc4-c343-5226f0a3b278"
      },
      "source": [
        "test_set = dependency_treebank.parsed_sents()[-10:]\n",
        "parses = origninal_tp.parse(test_set, 'original_tp.model')\n",
        "de = DependencyEvaluator(parses, test_set)\n",
        "las, uas = de.eval()\n",
        "\n",
        "print(las)\n",
        "print(uas)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8083333333333333\n",
            "0.8083333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80UieR6O1xER"
      },
      "source": [
        "### Custom Parser Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X__ZPoaPIYg"
      },
      "source": [
        "class CustomConfig(Configuration):\n",
        "\n",
        "    def extract_features(self):\n",
        "        result = []\n",
        "        # Todo : can come up with more complicated features set for better\n",
        "        # performance.\n",
        "        if len(self.stack) > 0:\n",
        "            # Stack 0\n",
        "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
        "            token = self._tokens[stack_idx0]\n",
        "            if self._check_informative(token[\"word\"], True):\n",
        "                result.append(\"STK_0_FORM_\" + token[\"word\"])\n",
        "            if \"lemma\" in token and self._check_informative(token[\"lemma\"]):\n",
        "                result.append(\"STK_0_LEMMA_\" + token[\"lemma\"])\n",
        "            if self._check_informative(token[\"tag\"]):\n",
        "                result.append(\"STK_0_POS_\" + token[\"tag\"])\n",
        "            if \"feats\" in token and self._check_informative(token[\"feats\"]):\n",
        "                feats = token[\"feats\"].split(\"|\")\n",
        "                for feat in feats:\n",
        "                    result.append(\"STK_0_FEATS_\" + feat)\n",
        "            \n",
        "            ###################### Head check ###################### \n",
        "            # hd = self._tokens.get_by_address(token[\"head\"]) \n",
        "            hd = self._tokens[token[\"head\"]][\"word\"]\n",
        "            if self._check_informative(hd):\n",
        "                result.append(\"STK_0_HEAD_\" + hd)\n",
        "            ###################### ######################\n",
        "\n",
        "            # Stack 1\n",
        "            if len(self.stack) > 1:\n",
        "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
        "                token = self._tokens[stack_idx1]\n",
        "                if self._check_informative(token[\"tag\"]):\n",
        "                    result.append(\"STK_1_POS_\" + token[\"tag\"])\n",
        "\n",
        "            # Left most, right most dependency of stack[0]\n",
        "            left_most = 1000000\n",
        "            right_most = -1\n",
        "            dep_left_most = \"\"\n",
        "            dep_right_most = \"\"\n",
        "            for (wi, r, wj) in self.arcs:\n",
        "                if wi == stack_idx0:\n",
        "                    if (wj > wi) and (wj > right_most):\n",
        "                        right_most = wj\n",
        "                        dep_right_most = r\n",
        "                    if (wj < wi) and (wj < left_most):\n",
        "                        left_most = wj\n",
        "                        dep_left_most = r\n",
        "            if self._check_informative(dep_left_most):\n",
        "                result.append(\"STK_0_LDEP_\" + dep_left_most)\n",
        "            if self._check_informative(dep_right_most):\n",
        "                result.append(\"STK_0_RDEP_\" + dep_right_most)\n",
        "\n",
        "        # Check Buffered 0\n",
        "        if len(self.buffer) > 0:\n",
        "            # Buffer 0\n",
        "            buffer_idx0 = self.buffer[0]\n",
        "            token = self._tokens[buffer_idx0]\n",
        "            if self._check_informative(token[\"word\"], True):\n",
        "                result.append(\"BUF_0_FORM_\" + token[\"word\"])\n",
        "            if \"lemma\" in token and self._check_informative(token[\"lemma\"]):\n",
        "                result.append(\"BUF_0_LEMMA_\" + token[\"lemma\"])\n",
        "            if self._check_informative(token[\"tag\"]):\n",
        "                result.append(\"BUF_0_POS_\" + token[\"tag\"])\n",
        "            if \"feats\" in token and self._check_informative(token[\"feats\"]):\n",
        "                feats = token[\"feats\"].split(\"|\")\n",
        "                for feat in feats:\n",
        "                    result.append(\"BUF_0_FEATS_\" + feat)\n",
        "            \n",
        "            ###################### Head check ######################\n",
        "            # hd = self._tokens.get_by_address(token[\"head\"]) \n",
        "            hd = self._tokens[token[\"head\"]][\"word\"]\n",
        "            if self._check_informative(hd):\n",
        "                result.append(\"BUF_0_HEAD_\" + hd)\n",
        "            ###################### ######################\n",
        "\n",
        "            # Buffer 1\n",
        "            if len(self.buffer) > 1:\n",
        "                buffer_idx1 = self.buffer[1]\n",
        "                token = self._tokens[buffer_idx1]\n",
        "                if self._check_informative(token[\"word\"], True):\n",
        "                    result.append(\"BUF_1_FORM_\" + token[\"word\"])\n",
        "                if self._check_informative(token[\"tag\"]):\n",
        "                    result.append(\"BUF_1_POS_\" + token[\"tag\"])\n",
        "            if len(self.buffer) > 2:\n",
        "                buffer_idx2 = self.buffer[2]\n",
        "                token = self._tokens[buffer_idx2]\n",
        "                if self._check_informative(token[\"tag\"]):\n",
        "                    result.append(\"BUF_2_POS_\" + token[\"tag\"])\n",
        "            if len(self.buffer) > 3:\n",
        "                buffer_idx3 = self.buffer[3]\n",
        "                token = self._tokens[buffer_idx3]\n",
        "                if self._check_informative(token[\"tag\"]):\n",
        "                    result.append(\"BUF_3_POS_\" + token[\"tag\"])\n",
        "            \n",
        "            # Left most, right most dependency of buff[0]\n",
        "            left_most = 1000000\n",
        "            right_most = -1\n",
        "            dep_left_most = \"\"\n",
        "            dep_right_most = \"\"\n",
        "            for (wi, r, wj) in self.arcs:\n",
        "                if wi == buffer_idx0:\n",
        "                    if (wj > wi) and (wj > right_most):\n",
        "                        right_most = wj\n",
        "                        dep_right_most = r\n",
        "                    if (wj < wi) and (wj < left_most):\n",
        "                        left_most = wj\n",
        "                        dep_left_most = r\n",
        "            if self._check_informative(dep_left_most):\n",
        "                result.append(\"BUF_0_LDEP_\" + dep_left_most)\n",
        "            if self._check_informative(dep_right_most):\n",
        "                result.append(\"BUF_0_RDEP_\" + dep_right_most)\n",
        "\n",
        "        return result\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g9kjjaR8ooJ"
      },
      "source": [
        "class CustomParser(TransitionParser):\n",
        "\n",
        "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
        "        \"\"\"\n",
        "        Create the training example in the libsvm format and write it to the input_file.\n",
        "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
        "        \"\"\"\n",
        "        operation = Transition(self.ARC_STANDARD)\n",
        "        count_proj = 0\n",
        "        training_seq = []\n",
        "\n",
        "        for depgraph in depgraphs:\n",
        "            if not self._is_projective(depgraph):\n",
        "                continue\n",
        "\n",
        "            count_proj += 1\n",
        "            conf = CustomConfig(depgraph)\n",
        "            while len(conf.buffer) > 0:\n",
        "                b0 = conf.buffer[0]\n",
        "                features = conf.extract_features()\n",
        "                binary_features = self._convert_to_binary_features(features)\n",
        "\n",
        "                if len(conf.stack) > 0:\n",
        "                    s0 = conf.stack[len(conf.stack) - 1]\n",
        "                    # Left-arc operation\n",
        "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
        "                    if rel is not None:\n",
        "                        key = Transition.LEFT_ARC + \":\" + rel\n",
        "                        self._write_to_file(key, binary_features, input_file)\n",
        "                        operation.left_arc(conf, rel)\n",
        "                        training_seq.append(key)\n",
        "                        continue\n",
        "\n",
        "                    # Right-arc operation\n",
        "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
        "                    if rel is not None:\n",
        "                        precondition = True\n",
        "                        # Get the max-index of buffer\n",
        "                        maxID = conf._max_address\n",
        "\n",
        "                        for w in range(maxID + 1):\n",
        "                            if w != b0:\n",
        "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
        "                                if relw is not None:\n",
        "                                    if (b0, relw, w) not in conf.arcs:\n",
        "                                        precondition = False\n",
        "\n",
        "                        if precondition:\n",
        "                            key = Transition.RIGHT_ARC + \":\" + rel\n",
        "                            self._write_to_file(key, binary_features, input_file)\n",
        "                            operation.right_arc(conf, rel)\n",
        "                            training_seq.append(key)\n",
        "                            continue\n",
        "\n",
        "                # Shift operation as the default\n",
        "                key = Transition.SHIFT\n",
        "                self._write_to_file(key, binary_features, input_file)\n",
        "                operation.shift(conf)\n",
        "                training_seq.append(key)\n",
        "\n",
        "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
        "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
        "        return training_seq\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSUCbhw012ER"
      },
      "source": [
        "### Train & Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOLd0Vs6p9VK",
        "outputId": "8eaa1c2c-b9a7-4668-ffde-bda55eca5353"
      },
      "source": [
        "custom_tp = CustomParser('arc-standard')\n",
        "custom_tp.train(sentences, 'custom_tp.model')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Number of training examples : 150\n",
            " Number of valid (projective) examples : 150\n",
            "[LibSVM]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxJhyMIi1r2i",
        "outputId": "72594031-d49d-4e2b-d3f9-02407c7dba95"
      },
      "source": [
        "custom_parses = custom_tp.parse(test_set, 'custom_tp.model')\n",
        "custom_de = DependencyEvaluator(custom_parses, test_set)\n",
        "las, uas = de.eval()\n",
        "\n",
        "print(las)\n",
        "print(uas)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8083333333333333\n",
            "0.8083333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDi9Pn9bpzMF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}